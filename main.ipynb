{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Ensure that NLTK's stop words are downloaded\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def extract_claims_from_url(url):\n",
    "    # Make an HTTP GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML using Beautiful Soup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Find all claim elements\n",
    "        claims = soup.find_all(class_='claim-text')\n",
    "        \n",
    "        # Extract text from each claim and remove numbering and stop words\n",
    "        claim_texts = []\n",
    "        current_claim_number = None\n",
    "        current_claim_text = \"\"\n",
    "        for claim in claims:\n",
    "            text = claim.get_text(strip=True)  # Use strip=True to remove leading and trailing whitespace\n",
    "            # Check if the claim is not a dependent claim\n",
    "            if 'dependent' not in claim.get('class', []):\n",
    "                claim_number_match = re.match(r'^(\\d+)\\.', text)\n",
    "                if claim_number_match:\n",
    "                    # If current claim text is not empty, append it to claim_texts\n",
    "                    if current_claim_text:\n",
    "                        filtered_text = ' '.join(word for word in current_claim_text.split() if word.lower() not in stop_words)\n",
    "                        claim_texts.append(filtered_text.strip())\n",
    "                    # Update current claim number and reset current claim text\n",
    "                    current_claim_number = int(claim_number_match.group(1))\n",
    "                    current_claim_text = text\n",
    "                else:\n",
    "                    # Concatenate the text to the current claim text\n",
    "                    current_claim_text += \" \" + text\n",
    "        # Append the last claim text\n",
    "        if current_claim_text:\n",
    "            filtered_text = ' '.join(word for word in current_claim_text.split() if word.lower() not in stop_words)\n",
    "            claim_texts.append(filtered_text.strip())\n",
    "        \n",
    "        return claim_texts\n",
    "    else:\n",
    "        # Print an error message if the request was not successful\n",
    "        print(f\"Failed to fetch HTML content from {url}. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "def extract_claims_from_multiple_urls(urls):\n",
    "    all_claims = []\n",
    "    for url in urls:\n",
    "        claims = extract_claims_from_url(url)\n",
    "        if claims:\n",
    "            all_claims.append(claims)\n",
    "    return all_claims\n",
    "\n",
    "# Example usage\n",
    "urls = [\n",
    "    \"https://patents.google.com/patent/GB2478972A/en?q=(phone)&oq=phone\",\n",
    "    \"https://patents.google.com/patent/US9634864B2/en?oq=US9634864B2\",\n",
    "    \"https://patents.google.com/patent/US9980046B2/en?oq=US9980046B2\"\n",
    "]  # Replace \"URL1\", \"URL2\", etc. with your actual URLs\n",
    "all_claims = extract_claims_from_multiple_urls(urls)\n",
    "\n",
    "for url, claims in zip(urls, all_claims):\n",
    "    print(url)\n",
    "    for claim_text in claims:\n",
    "        print(claim_text)\n",
    "        print(\"--------------\")\n",
    "print(len(all_claims))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from Kmeans import Kmeans\n",
    "\n",
    "\n",
    "# Flatten the list of lists into a single list\n",
    "all_claims_flat = [claim for sublist in all_claims for claim in sublist]\n",
    "\n",
    "# Convert claims to lowercase\n",
    "all_claims_lower = [claim.lower() for claim in all_claims_flat]\n",
    "\n",
    "\n",
    "num_clusters = 5\n",
    "normal_kmeans = Kmeans(num_clusters, all_claims_lower, all_claims_flat)\n",
    "normal_kmeans.print_clustered_claims()\n",
    "normal_kmeans.plot()\n",
    "\n",
    "\n",
    "print(\"distances of clusters\\n\",normal_kmeans.distance_between_clusters())\n",
    "\n",
    "print(\"clusters density\\n\",normal_kmeans.cluster_density())\n",
    "\n",
    "\n",
    "#done using TF-IDF\n",
    "\n",
    "print(normal_kmeans.get_clusters_titles())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from GMMClustering import GMMClustering\n",
    "\n",
    "gmm = GMMClustering(5,all_claims_lower,all_claims_flat)\n",
    "gmm.plot()\n",
    "gmm.print_clustered_claims()\n",
    "\n",
    "print(\"distances of clusters\\n\",gmm.distance_between_clusters())\n",
    "\n",
    "print(\"clusters density\\n\",gmm.cluster_density())\n",
    "\n",
    "#using czearing title model generator\n",
    "\n",
    "print(gmm.get_clusters_titles())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GPT2Kmeans import GPT2Kmeans\n",
    "gpt2_kmeans = GPT2Kmeans(num_clusters, all_claims_flat)\n",
    "\n",
    "# Plot clusters\n",
    "gpt2_kmeans.plot()\n",
    "\n",
    "# Print clustered claims\n",
    "gpt2_kmeans.print_clustered_claims()\n",
    "\n",
    "\n",
    "\n",
    "print(\"distances of clusters\\n\", gpt2_kmeans.distance_between_clusters())\n",
    "print(\"clusters density\\n\", gpt2_kmeans.cluster_density())\n",
    "\n",
    "# done by embeddings\n",
    "print(gpt2_kmeans.get_clusters_titles())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i will choose the GMM with te model because its using a smarter clustering algorithem with a language model to create titles based\n",
    "# on other examples in\n",
    "# the biggest advatage of that way is the \"accuracy\" of the clustering titles. but its a really slow method thats might require resorces\n",
    "# like gpu and even then might take alot of time on larger data.\n",
    "\n",
    "# in terms of performance and accuracy i will probably choose the gpt2 with embeddings. even tho its also using an llm to generate titles\n",
    "# its uses the llm inorder to cluster and then using the embeddings to get a title (embeddings after llm promies good embeddings and much quicker)\n",
    "\n",
    "#overall winner is GPT2 with embeddings."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
